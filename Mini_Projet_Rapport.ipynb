{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Projet Multimédia: Segementation audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Becko Camara; Guobao LI; Rongbo Liu; Shiting LI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But: \n",
    "      Reconnaissance de locuteurs dans une bande audio et détection de changement de locuteurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données: \n",
    "               Bande audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outils:\n",
    "          1. pyAudioAnalysis\n",
    "          2. Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Concepiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Déterminer la taille de la fênetre et la taille du bloc.\n",
    "\n",
    "- Extraction des caractéristiques \n",
    "On utilise readAudioFile() de la bibliothèque pour lire le fichier audio, puis obtenir sa fréquence d'échantillonage et le vecteur qui lui correspond. Ensuite, avec la méthode stFeatuureExtraction(), on extrait les caractéristiques de la bande audio sur des fenêtres de 50 ms qu'on incrémentées de 25 ms\n",
    "\n",
    "- Calcul des MFCCs \n",
    "MFCC pour Mel Frequency Cepstrum Coefficient, sont des coefficients basés sur les caractéristiques vocales humaines. il y a une correspondance non linéaire entre féquence mel et fréquence Hz. Mel Frequency Cepstral Coefficients (MFCC) est l'utilisation d'une telle relation entre eux, calculer les caractéristiques des spectrales HZ. Nous avons donc utilisé MFCC pour la reconnaissance vocale.\n",
    "\n",
    "- Calculer la distribution de gauss de deux blocs consécutifs \n",
    "\n",
    "- On combine ces deux blocs, et on calcule la distribution de gauss du bloc combiné\n",
    "\n",
    "- On stocke le résultat dans le tableaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Realisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le réalisation de  code python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des dépendances et les bibliothèques néssaires:\n",
    "\n",
    "- audioBasicIO\n",
    "\n",
    "- audioFeatureExtraction\n",
    "\n",
    "- numpy(stockage et manutention des grandes matrices)\n",
    "\n",
    "- math\n",
    "\n",
    "- matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/scipy/io/wavfile.py:42: WavFileWarning: Unknown wave file format\n",
      "  warnings.warn(\"Unknown wave file format\", WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "# main process\n",
    "[Fs, x] = audioBasicIO.readAudioFile(\"data/diarizationExample.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- audioBasicIO.readAudioFile(path)\n",
    "Cette fonction retourne un tableau numpy qui stocke l'échantillon audio d'un WAV.\n",
    "\n",
    "*Il y a une warnning ici car le format de wav.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TIME_OF_WINDOW = 0.050\t#a window = 0.05s\n",
    "TIME_OF_STEP = 0.025\t\t#step = 0.01s\n",
    "SIZE_OF_WINDOW = int(TIME_OF_WINDOW * Fs)\t#the number of frame for one window\n",
    "SIZE_OF_STEP = int(TIME_OF_STEP * Fs)\t\t#the number of frame for one step\n",
    "BLOCK_SIZE = 4\t\t#a block has (6 * SIZE_OF_STEP) frame\n",
    "BLOCK_STEP = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Determiner le fênetre et le bloc\n",
    "\n",
    "- Une taille de fenêtre de 50 ms et une étape de 25 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variables \n",
    "END_OF_FILE = 0\n",
    "FIRST_PAIR = 1\n",
    "INDEX_BOUCLE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getMFCCs(block_start, block_end):\n",
    "\treturn attribute[8:20,block_start:block_end+1]\n",
    "\n",
    "def getMFCCsFromTime(moment_start, moment_end):\n",
    "\tblock_start = int(moment_start / BLOCK_STEP / TIME_OF_STEP - 1)\n",
    "\tblock_end = int(moment_end / BLOCK_STEP / TIME_OF_STEP - 1)\n",
    "\treturn getMFCCs(block_start, block_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Calculer les MFCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gauss(x, mean, cov):\n",
    "\t[n, d] = x.shape\n",
    "\t[j, k] = cov.shape\n",
    "\tif (j != n) | (k != n):\n",
    "\t\traise Exception(\"Dimension of the covariance matrix and data should match\")\n",
    "\tinvcov = cov.T\n",
    "\tmean = np.reshape(mean, (1, n))\n",
    "\n",
    "\tx = x - (np.ones((d, 1))*mean).T\n",
    "\tfact = np.sum(((np.dot(invcov, x))*x), axis = 1)\n",
    "\n",
    "\ty = np.exp(-0.5*fact)\n",
    "\n",
    "\ty = np.divide(y, math.pow((2*math.pi), n)*np.std(cov))\n",
    "\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Définir la méthode de Gauss pour calculer la distribution de gauss des deux blocs consécutifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature extraction from the library pyAudioAnalysis\n",
    "attribute = audioFeatureExtraction.stFeatureExtraction(x, Fs, SIZE_OF_WINDOW, SIZE_OF_STEP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature extraction\n",
    "\n",
    "- stFeatureExtraction()\n",
    "pour extraire les courtes terme feature séquencesde pour un signal audio, en utilisant une taille de frame de 50 ms et une étape de frame de 25 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# relationship between the similarity and the timestamp in the audio\n",
    "relation = [[1 for col in range(2)] for row in range(attribute.shape[1]/BLOCK_STEP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while (END_OF_FILE == 0):\n",
    "\tif (FIRST_PAIR == 1):\n",
    "\t\t# for the first pari of the block\n",
    "\t\tblock_i_index_start = 0\n",
    "\t\tblock_i_index_end = BLOCK_SIZE\n",
    "\t\tblock_i_attribute = getMFCCs(block_i_index_start, block_i_index_end)\n",
    "\n",
    "\n",
    "\t\tblock_i_mean = np.mean(block_i_attribute, axis=1)\n",
    "\t\tblock_i_cov = np.cov(block_i_attribute)\n",
    "\t\tblock_i_log_like = np.log(gauss(block_i_attribute, mean=block_i_mean, cov=block_i_cov))\n",
    "\n",
    "\n",
    "\n",
    "\t\tblock_j_index_start = block_i_index_end + 1\n",
    "\t\tblock_j_index_end = block_j_index_start + BLOCK_SIZE - 1\n",
    "\t\tblock_j_attribute = getMFCCs(block_j_index_start, block_j_index_end)\n",
    "\n",
    "\n",
    "\t\tblock_j_mean = np.mean(block_j_attribute, axis=1)\n",
    "\t\tblock_j_cov = np.cov(block_j_attribute)\n",
    "\t\tblock_j_log_like = np.log(gauss(block_j_attribute, mean=block_j_mean, cov=block_j_cov))\n",
    "\n",
    "\n",
    "\t\tFIRST_PAIR = 0\n",
    "        \n",
    "        else:\n",
    "\t\t#for the rest of the block\n",
    "\t\tblock_j_index_start += BLOCK_STEP\n",
    "\t\tblock_j_index_end += BLOCK_STEP\n",
    "\n",
    "\t\tnew_attribute = getMFCCs(block_j_index_end-BLOCK_STEP+1, block_j_index_end)\n",
    "\n",
    "\t\t#the following code is for the object that to avoid recalculate the overlap between the block after moved and before moved\n",
    "\t\tblock_i_index_start += BLOCK_STEP\n",
    "\t\tblock_i_index_end += BLOCK_STEP\n",
    "\t\tblock_i_attribute[:,0:block_i_attribute.shape[1]-new_attribute.shape[1]] = block_i_attribute[:,new_attribute.shape[1]:block_i_attribute.shape[1]]\n",
    "\t\tblock_i_attribute[:,block_i_attribute.shape[1]-new_attribute.shape[1]:block_i_attribute.shape[1]] = block_j_attribute[:,0:new_attribute.shape[1]]\n",
    "\n",
    "\t\tblock_i_mean = np.mean(block_i_attribute, axis=1)\n",
    "\t\tblock_i_cov = np.cov(block_i_attribute)\n",
    "\t\tblock_i_log_like = np.log(gauss(block_i_attribute, mean=block_i_mean, cov=block_i_cov))\n",
    "\n",
    "\n",
    "\t\tblock_j_attribute[:,0:block_j_attribute.shape[1]-new_attribute.shape[1]] = block_j_attribute[:,new_attribute.shape[1]:block_j_attribute.shape[1]]\n",
    "\t\tblock_j_attribute[:,block_j_attribute.shape[1]-new_attribute.shape[1]:block_j_attribute.shape[1]] = new_attribute[:,0:new_attribute.shape[1]]\n",
    "\n",
    "\t\tblock_j_mean = np.mean(block_j_attribute, axis=1)\n",
    "\t\tblock_j_cov = np.cov(block_j_attribute)\n",
    "\t\tblock_j_log_like = np.log(gauss(block_j_attribute, mean=block_j_mean, cov=block_j_cov))\n",
    "\n",
    "\n",
    "\tblock_union_index_start = block_i_index_start\n",
    "\tblock_union_index_end = block_j_index_end\n",
    "\tblock_union_attribute = np.concatenate((block_i_attribute, block_j_attribute), axis = 1)\n",
    "\tblock_union_mean = np.mean(block_union_attribute, axis=1)\n",
    "\tblock_union_cov = np.cov(block_union_attribute)\n",
    "\tblock_union_log_like = np.log(gauss(block_union_attribute, mean=block_union_mean, cov=block_union_cov))\n",
    "\n",
    "\trelation[INDEX_BOUCLE-1][0] = np.sum(block_i_log_like) + np.sum(block_j_log_like) - np.sum(block_union_log_like)\n",
    "\trelation[INDEX_BOUCLE-1][1] = (block_i_index_end + block_j_index_start) / 2 * TIME_OF_STEP\n",
    "\n",
    "\n",
    "\tINDEX_BOUCLE += 1\n",
    "\n",
    "\tif block_j_index_end + BLOCK_STEP > attribute.shape[1]:\n",
    "\t\tEND_OF_FILE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Processus de calcul pour la première partie d'un bloc.\n",
    "\n",
    "- On calcule les MFCCs de block_i, ensuite on calcule la valeur moyenne des MFCCs, la valeur de covariance des MFCCs, la valeur de Gausse entre les MFCCs, la valeur moyenne des MFCCs et la valeur de covariance des MFCCs.\n",
    "\n",
    "- Pour le block_j, on fait les mêmes calculs comme block_i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On continue à faire les calculs pour la restante partie de bloc.\n",
    "\n",
    "- Nous avons constamment procédé alternatif pour éviter les calculs répétitifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========The data after cutting===========\n",
      "[193.04016786970868, 0.1]\n",
      "[322.80179095213964, 0.15000000000000002]\n",
      "[48.618235376979669, 3.75]\n",
      "[206.06284495930311, 3.8000000000000003]\n",
      "[264.46484990545093, 5.800000000000001]\n",
      "[399.62976041628497, 6.7]\n",
      "[185.27945423237873, 6.75]\n",
      "[77.58484185258385, 11.05]\n",
      "[55.7106174875737, 12.700000000000001]\n",
      "[86.606563878601378, 13.100000000000001]\n",
      "[65.574334156685495, 16.6]\n",
      "[25.698883507655694, 18.75]\n",
      "[119.17666505648083, 18.85]\n",
      "[157.12783806256266, 18.900000000000002]\n",
      "[82.460244094641666, 24.5]\n",
      "[10.571385024471169, 25.150000000000002]\n",
      "[1.5434625659503354, 25.25]\n",
      "[211.66294698210436, 34.2]\n",
      "[70.795005105859445, 34.35]\n",
      "[100.16974802846812, 34.4]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "# cut the audio\n",
    "relation_cut = filter(lambda t: t[0] > 0, relation)\n",
    "\n",
    "print \"=========The data after cutting===========\"\n",
    "for item in relation_cut:\n",
    "\tprint item\n",
    "\n",
    "\n",
    "\n",
    "# print getMFCCsFromTime(0.4, 4.0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Couper l'audio et délivre le résultat.\n",
    "\n",
    "- Si la valeur de Gauss est négative, c'est-à-dire que ce clip d'audio est la même personne comme le fragment précédent. Et si ls valeur de Gauss est positive, c'est-à-dire que ce clip d'audio n'est pas la même personne comme le fragment précédent, on va faire une coupe ici.\n",
    "\n",
    "- Nous ignorons tous les valeurs de Gauss négatives, seule interceptons les valeurs de Gauss positives.\n",
    "\n",
    "- Pour le resultat [x, y], x est le résultat de la mise en oeuvre de Gauss, y est le point de temps de couper l'audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reconnaissance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1 Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le conception de faire une reconnaissance est utiliser k-means pour mettre les block similaire ensemble.\n",
    "- Le premier fonction **euclDistance(vector1, vector2)** est calculer la distance entre deux vector.\n",
    "- Le deuxième foction **initCentroids(dataSet, k)** est obtenir k centres sur le data. **k** est le nombre de class. Dans ce projet, le nombre de class est le nombre des locuteurs.\n",
    "- Le troisième fonction **kmeans(dataSet, k)** est utiliser le distance Euclidean pour mettre les points ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#===========Kmeans Definition=====================\n",
    "#calculate Euclidean distance\n",
    "def euclDistance(vector1, vector2):\n",
    "\treturn sqrt(sum(power(vector2 - vector1, 2)))\n",
    "\n",
    "# init centroids with random samples\n",
    "def initCentroids(dataSet, k):\n",
    "\tnumSamples, dim = dataSet.shape\n",
    "\tcentroids = zeros((k, dim))\n",
    "\tfor i in range(k):\n",
    "\t\tindex = int(random.uniform(0, numSamples))\n",
    "\t\tcentroids[i, :] = dataSet[index, :]\n",
    "\treturn centroids\n",
    "\n",
    "# k-means cluster\n",
    "def kmeans(dataSet, k):\n",
    "\tnumSamples = dataSet.shape[0]\n",
    "\t# first column stores which cluster this sample belongs to,\n",
    "\t# second column stores the error between this sample and its centroid\n",
    "\tclusterAssment = mat(zeros((numSamples, 2)))\n",
    "\tclusterChanged = True\n",
    "\n",
    "\t## step 1: init centroids\n",
    "\tcentroids = initCentroids(dataSet, k)\n",
    "\n",
    "\twhile clusterChanged:\n",
    "\t\tclusterChanged = False\n",
    "\t\t## for each sample\n",
    "\t\tfor i in xrange(numSamples):\n",
    "\t\t\tminDist  = 100000.0\n",
    "\t\t\tminIndex = 0\n",
    "\t\t\t## for each centroid\n",
    "\t\t\t## step 2: find the centroid who is closest\n",
    "\t\t\tfor j in range(k):\n",
    "\t\t\t\tdistance = euclDistance(centroids[j, :], dataSet[i, :])\n",
    "\t\t\t\tif distance < minDist:\n",
    "\t\t\t\t\tminDist  = distance\n",
    "\t\t\t\t\tminIndex = j\n",
    "\n",
    "\t\t\t## step 3: update its cluster\n",
    "\t\t\tif clusterAssment[i, 0] != minIndex:\n",
    "\t\t\t\tclusterChanged = True\n",
    "\t\t\t\tclusterAssment[i, :] = minIndex, minDist**2\n",
    "\n",
    "\t\t## step 4: update centroids\n",
    "\t\tfor j in range(k):\n",
    "\t\t\tpointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]]\n",
    "\t\t\tcentroids[j, :] = mean(pointsInCluster, axis = 0)\n",
    "\n",
    "\tprint 'cluster complete!'\n",
    "\treturn centroids, clusterAssment\n",
    "\n",
    "def abs (input):\n",
    "\tif input <0 : return - input\n",
    "\telse : return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dans cette étap, on d'abord créer une novelle liste *temprelation* qui prendre le premier colonne de relation_cut et ajouter un deuxième colonne qui a des valeurs égal 1. \n",
    "\n",
    "- Ensuite, on transmettre cette liste à une matrice par utilise ```dataSet= mat(dataSet)```.\n",
    "\n",
    "- Enfin, on utlise cette matrice pour calculer les centres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "step 1: load data...\n",
      "step 2: clustering...\n",
      "cluster complete!\n"
     ]
    }
   ],
   "source": [
    "print '======='\n",
    "\n",
    "temprelation = []\n",
    "for i in range(len(relation_cut)-3): #-3 because the last 3 numbers are meaningless\n",
    "    temprelation.append([relation_cut[i][0],1])\n",
    "\n",
    "#temprelation is a list which takes only first colum of relation_cut and add another colum which values are all 1\n",
    "#This action alow to transefer the MFCC to a 2-dimension data which use for cluster\n",
    "\n",
    "\n",
    "## step 1: load data\n",
    "print \"step 1: load data...\"\n",
    "dataSet = temprelation\n",
    "\n",
    "\n",
    "# step 2: clustering...\n",
    "print \"step 2: clustering...\"\n",
    "dataSet = mat(dataSet)\n",
    "k = 4\n",
    "centroids, clusterAssment = kmeans(dataSet, k)\n",
    "classification =[]\n",
    "for indexi in range(k):\n",
    "\tclassification .append(centroids[indexi][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour chaque block, on calculer le distance avec le centre et le point, et marker different locuteur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## step 3: Mark speaker to each block\n",
    "relation_cut_regonize = [] #This list is to store the information with different speakers\n",
    "for indexj in range(len(dataSet)):\n",
    "\tmin1 = abs(relation_cut[indexj][0]-classification[0])\n",
    "\tmin2 = abs(relation_cut[indexj][0]-classification[1])\n",
    "\tmin3 = abs(relation_cut[indexj][0]-classification[2])\n",
    "\tmin4 = abs(relation_cut[indexj][0]-classification[3])\n",
    "\tif min1<min2 and min1 < min3 and min1<min4:\n",
    "\t\trelation_cut_regonize.append([relation_cut[indexj],\"Speaker1\"])\n",
    "\telse:\n",
    "\t\tif min2<min1 and min2 < min3 and min2<min4:\n",
    "\t\t\trelation_cut_regonize.append([relation_cut[indexj],\"Speaker2\"])\n",
    "\t\telse:\n",
    "\t\t\tif min3<min1 and min3 < min2 and min3<min4:\n",
    "\t\t\t\trelation_cut_regonize.append([relation_cut[indexj],\"Speaker3\"])\n",
    "\t\t\telse:\n",
    "\t\t\t\tif min4<min1 and min4 < min2 and min4<min3:\n",
    "\t\t\t\t\trelation_cut_regonize.append([relation_cut[indexj],\"Speaker4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On afficher le result de reconnaissance avec une iterateur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========The data after regonizing as 4 speakers===========\n",
      "[[193.04016786970868, 0.1], 'Speaker4']\n",
      "[[322.80179095213964, 0.15000000000000002], 'Speaker3']\n",
      "[[48.618235376979669, 3.75], 'Speaker2']\n",
      "[[206.06284495930311, 3.8000000000000003], 'Speaker4']\n",
      "[[264.46484990545093, 5.800000000000001], 'Speaker3']\n",
      "[[399.62976041628497, 6.7], 'Speaker3']\n",
      "[[185.27945423237873, 6.75], 'Speaker4']\n",
      "[[77.58484185258385, 11.05], 'Speaker2']\n",
      "[[55.7106174875737, 12.700000000000001], 'Speaker2']\n",
      "[[86.606563878601378, 13.100000000000001], 'Speaker2']\n",
      "[[65.574334156685495, 16.6], 'Speaker2']\n",
      "[[25.698883507655694, 18.75], 'Speaker1']\n",
      "[[119.17666505648083, 18.85], 'Speaker2']\n",
      "[[157.12783806256266, 18.900000000000002], 'Speaker4']\n",
      "[[82.460244094641666, 24.5], 'Speaker2']\n",
      "[[10.571385024471169, 25.150000000000002], 'Speaker1']\n",
      "[[1.5434625659503354, 25.25], 'Speaker1']\n",
      "[[211.66294698210436, 34.2], 'Speaker4']\n",
      "[[70.795005105859445, 34.35], 'Speaker2']\n",
      "[[100.16974802846812, 34.4], 'Speaker2']\n"
     ]
    }
   ],
   "source": [
    "#step4: shows the result\n",
    "print \"=========The data after regonizing as 4 speakers===========\"\n",
    "for item in relation_cut_regonize:\n",
    "\tprint item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.  Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce mini-projet, nous avons appris à segmenter un fichier audio au format .wav. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pour la partie de segmentation. Nous avons réussi à couper l'audio. Cependant, nos résultats ne sont pas très précis, et nous soupçonnons cela est un problème de paramètre. Nous définissons le paramètre était trop sensible. Donc, nous avons mis beaucoup de coupe audio en petits morceaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cependant, en regroupant la reconnaissance, nous avons finalement réussi à identifier les quatre personnes à l'intérieur de ce discours audio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
